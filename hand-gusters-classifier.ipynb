{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fd0709-ab30-4821-9bc1-b575a2bd4a7a",
   "metadata": {},
   "source": [
    "# Installing OpenCV and Python Dependencies\n",
    "\n",
    "## Install OpenCV\n",
    "To install OpenCV using pip, run the following command:\n",
    "```bash\n",
    "pip install opencv-python\n",
    "```\n",
    "\n",
    "If you need OpenCV with additional functionalities like `opencv-contrib-python`, install:\n",
    "```bash\n",
    "pip install opencv-contrib-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ca1161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    }
   ],
   "source": [
    "print(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d67a082-1c8a-4c11-9f07-265f9c254075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "if not capture.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = capture.read()  # Capture frame-by-frame\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('Camera Feed', frame)  # Display the frame\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84703fea-d457-4a49-a687-22b100df173a",
   "metadata": {},
   "source": [
    "## Install MediaPipe\n",
    "MediaPipe is required for hand tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c777c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ffc73d-0d4c-4eb2-8f7e-bbe58bc0a08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (3.10.1)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (4.25.6)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from jax->mediapipe) (1.15.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python3.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62b4102a-df1c-40ff-ac9e-40556f69ac4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drawing is done: Distance = 0.158\n",
      "Drawing is done: Distance = 0.177\n",
      "Drawing is done: Distance = 0.238\n",
      "Drawing is done: Distance = 0.238\n",
      "Drawing is done: Distance = 0.191\n",
      "Drawing is done: Distance = 0.123\n",
      "Drawing is done: Distance = 0.076\n",
      "Drawing is done: Distance = 0.141\n",
      "Drawing is done: Distance = 0.226\n",
      "Drawing is done: Distance = 0.287\n",
      "Drawing is done: Distance = 0.230\n",
      "Drawing is done: Distance = 0.100\n",
      "Drawing is done: Distance = 0.114\n",
      "Drawing is done: Distance = 0.278\n",
      "Drawing is done: Distance = 0.293\n",
      "Drawing is done: Distance = 0.296\n",
      "Drawing is done: Distance = 0.108\n",
      "Drawing is done: Distance = 0.068\n",
      "Drawing is done: Distance = 0.042\n",
      "Drawing is done: Distance = 0.136\n",
      "Drawing is done: Distance = 0.170\n",
      "Drawing is done: Distance = 0.258\n",
      "Drawing is done: Distance = 0.269\n",
      "Drawing is done: Distance = 0.327\n",
      "Drawing is done: Distance = 0.340\n",
      "Drawing is done: Distance = 0.376\n",
      "Drawing is done: Distance = 0.395\n",
      "Drawing is done: Distance = 0.396\n",
      "Drawing is done: Distance = 0.375\n",
      "Drawing is done: Distance = 0.305\n",
      "Drawing is done: Distance = 0.174\n",
      "Drawing is done: Distance = 0.144\n",
      "Drawing is done: Distance = 0.093\n",
      "Drawing is done: Distance = 0.080\n",
      "Drawing is done: Distance = 0.066\n",
      "Drawing is done: Distance = 0.184\n",
      "Drawing is done: Distance = 0.223\n",
      "Drawing is done: Distance = 0.261\n",
      "Drawing is done: Distance = 0.367\n",
      "Drawing is done: Distance = 0.387\n",
      "Drawing is done: Distance = 0.429\n",
      "Drawing is done: Distance = 0.428\n",
      "Drawing is done: Distance = 0.418\n",
      "Drawing is done: Distance = 0.408\n",
      "Drawing is done: Distance = 0.325\n",
      "Drawing is done: Distance = 0.207\n",
      "Drawing is done: Distance = 0.153\n",
      "Drawing is done: Distance = 0.077\n",
      "Drawing is done: Distance = 0.051\n",
      "Drawing is done: Distance = 0.048\n",
      "Drawing is done: Distance = 0.041\n",
      "Drawing is done: Distance = 0.045\n",
      "Drawing is done: Distance = 0.139\n",
      "Drawing is done: Distance = 0.148\n",
      "Drawing is done: Distance = 0.133\n",
      "Drawing is done: Distance = 0.051\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "def drawLine_landmark(frame, hand_landmarks):\n",
    "    index_finger_tip = hand_landmarks.landmark[8]\n",
    "    thumb_tip = hand_landmarks.landmark[4]\n",
    "\n",
    "    # Convert normalized coordinates (0-1) to pixel values\n",
    "    index_finger_x = int(index_finger_tip.x * frame.shape[1])\n",
    "    index_finger_y = int(index_finger_tip.y * frame.shape[0])\n",
    "    thumb_x = int(thumb_tip.x * frame.shape[1])\n",
    "    thumb_y = int(thumb_tip.y * frame.shape[0])\n",
    "\n",
    "    # Calculate Euclidean distance\n",
    "    dis = ((index_finger_tip.x - thumb_tip.x)**2 + (index_finger_tip.y - thumb_tip.y)**2)**0.5\n",
    "\n",
    "    # Calculate midpoint of the line\n",
    "    mid_x = (index_finger_x + thumb_x) // 2\n",
    "    mid_y = (index_finger_y + thumb_y) // 2\n",
    "\n",
    "    # Draw line\n",
    "    cv2.line(frame, (index_finger_x, index_finger_y), (thumb_x, thumb_y), (205, 55, 120), 5)\n",
    "\n",
    "    # Draw text at the midpoint\n",
    "    text = f\"{dis:.3f}\"\n",
    "    font_scale = 0.6\n",
    "    font_thickness = 2\n",
    "    text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)[0]\n",
    "    text_x = mid_x - text_size[0] // 2  # Center text horizontally\n",
    "    text_y = mid_y + text_size[1] // 2  # Center text vertically\n",
    "    cv2.putText(frame, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness, cv2.LINE_AA)\n",
    "\n",
    "    depth = index_finger_tip.z  # Get Z-value (depth)\n",
    "    text_Des = f\"Depth: {depth:.3f}\"  # Format to 3 decimal places\n",
    "    cv2.putText(frame, text_Des, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    print(f\"Drawing is done: Distance = {dis:.3f}\")\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Open the camera\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "if not capture.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        heightFrame, widthFrame, _ = frame.shape\n",
    "\n",
    "        # Convert image to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(rgb_frame)\n",
    "        \n",
    "        if result.multi_hand_landmarks:\n",
    "            for hand_landmarks, handedness in zip(result.multi_hand_landmarks, result.multi_handedness):\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                # Get hand label (Left or Right)\n",
    "                hand_label = handedness.classification[0].label\n",
    "                x, y = int(hand_landmarks.landmark[0].x * widthFrame), int(hand_landmarks.landmark[0].y * heightFrame)\n",
    "                cv2.putText(frame, f\"{hand_label} Hand\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "\n",
    "                # Draw line between index finger and thumb\n",
    "                drawLine_landmark(frame, hand_landmarks)\n",
    "\n",
    "        # Create a semi-transparent overlay for the red circle\n",
    "        overlay = frame.copy()\n",
    "        cv2.circle(overlay, (widthFrame // 2, heightFrame // 2), 75, (0, 0, 255), -1)  # Red filled circle\n",
    "        alpha = 0.5  # Transparency level\n",
    "        cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "\n",
    "        cv2.imshow('Hand Tracking', frame)  # Display the frame\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1329a82",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# I. Interface hands gusters\n",
    "make a class or interface that will take hands gusters and assign to each on of them a fucntion and then we can programe these fucntion as we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb42e1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 's' to start recording, and 'q' to quit.\n",
      "Starting recording...\n",
      "Recording finished.\n",
      "Starting recording...\n",
      "Recording finished.\n",
      "Starting recording...\n",
      "Recording finished.\n",
      "Starting recording...\n",
      "Recording finished.\n",
      "Starting recording...\n",
      "Recording finished.\n",
      "Starting recording...\n",
      "Recording finished.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class HandGesture:\n",
    "    def __init__(self, name, landmarks=None):\n",
    "        self.name = name\n",
    "        self.landmarks = landmarks if landmarks else []\n",
    "    \n",
    "    def set_landmarks(self, landmarks):\n",
    "        self.landmarks = landmarks\n",
    "    \n",
    "    def compare_gesture(self, detected_landmarks):\n",
    "        \"\"\"Compare the detected hand with the stored gesture.\"\"\"\n",
    "        if not self.landmarks or not detected_landmarks:\n",
    "            return False\n",
    "        \n",
    "        # Compute Euclidean distance between corresponding landmarks\n",
    "        distances = [np.linalg.norm(\n",
    "            np.array([self.landmarks[i].x, self.landmarks[i].y]) -\n",
    "            np.array([detected_landmarks[i].x, detected_landmarks[i].y])\n",
    "        ) for i in range(len(self.landmarks))]\n",
    "        \n",
    "        return np.mean(distances) < 0.05  # Adjust threshold as needed\n",
    "\n",
    "class HandDetector:\n",
    "    def __init__(self):\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.hands = self.mp_hands.Hands(static_image_mode=True, max_num_hands=1)\n",
    "    \n",
    "    def detect_hands(self, image):\n",
    "        results = self.hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        if not results.multi_hand_landmarks:\n",
    "            return None  # No hands detected\n",
    "        \n",
    "        return results.multi_hand_landmarks[0].landmark\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load the progress from the JSON file, or initialize if not found.\"\"\"\n",
    "    progress_file = 'progress.json'\n",
    "    \n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        # If no progress file, initialize it with the first class index\n",
    "        return {'last_class_index': 0}\n",
    "\n",
    "def save_progress(progress):\n",
    "    \"\"\"Save the current progress to the JSON file.\"\"\"\n",
    "    with open('progress.json', 'w') as f:\n",
    "        json.dump(progress, f)\n",
    "\n",
    "def create_class_folder(class_index):\n",
    "    dataset_dir = 'dataset'\n",
    "    class_folder = os.path.join(dataset_dir, f'class_{class_index}')\n",
    "    \n",
    "    # Check if class folder already exists, and if so, increment the class_index\n",
    "    while os.path.exists(class_folder):\n",
    "        class_index += 1\n",
    "        class_folder = os.path.join(dataset_dir, f'class_{class_index}')\n",
    "    \n",
    "    # Create the class folder\n",
    "    os.makedirs(class_folder)\n",
    "    return class_folder, class_index\n",
    "\n",
    "def capture_and_record(nbr_sample_per_gestures = 100):\n",
    "    progress = load_progress()  # Load the current progress\n",
    "    class_index = progress['last_class_index']  # Get the last used class index\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    detector = HandDetector()\n",
    "\n",
    "    class_folder, class_index = create_class_folder(class_index)\n",
    "    progress['last_class_index'] = class_index  # Update the progress\n",
    "    save_progress(progress)  # Save progress to JSON file\n",
    "    \n",
    "    frame_count = 0\n",
    "    num_frames = nbr_sample_per_gestures\n",
    "    recording = False\n",
    "    \n",
    "    print(\"Press 's' to start recording, and 'q' to quit.\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Create a copy of the frame for saving (without the text)\n",
    "        frame_to_save = frame.copy()\n",
    "        \n",
    "        # Detect hand landmarks\n",
    "        landmarks = detector.detect_hands(frame)\n",
    "        \n",
    "        if landmarks:\n",
    "            # Display recording status on the original frame (with text)\n",
    "            if recording:\n",
    "                cv2.putText(frame, \"Recording...\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                cv2.putText(frame, f\"Frames recorded: {frame_count}/{num_frames}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                \n",
    "                # Save the frame after every successful capture (without the text)\n",
    "                if frame_count < num_frames:\n",
    "                    frame_name = f\"{frame_count + 1}.jpg\"\n",
    "                    cv2.imwrite(os.path.join(class_folder, frame_name), frame_to_save)  # Save the frame without text\n",
    "                    frame_count += 1\n",
    "                \n",
    "                if frame_count >= num_frames:\n",
    "                    print(\"Recording finished.\")\n",
    "                    cv2.putText(frame, \"Recording Finished\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                    recording = False\n",
    "                    class_folder, class_index = create_class_folder(class_index)\n",
    "                    progress['last_class_index'] = class_index  # Update the progress\n",
    "                    save_progress(progress) \n",
    "            else:\n",
    "                # Show \"Press 's' to start\" message on the original frame\n",
    "                cv2.putText(frame, \"Press 's' to start recording\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(\"Hand Gesture Recorder\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('s'):  # Start recording when 's' is pressed\n",
    "            if not recording:\n",
    "                print(\"Starting recording...\")\n",
    "                recording = True\n",
    "                frame_count = 0  # Reset frame count\n",
    "        elif key == ord('q'):  # Quit when 'q' is pressed\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "capture_and_record(200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a731bbb",
   "metadata": {},
   "source": [
    "---\n",
    "## Proccess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6053b869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to gesture_data.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "\n",
    "DATA_DIR = 'dataset'  # Define the dataset directory\n",
    "\n",
    "# Initialize hand detector using MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1)\n",
    "\n",
    "def extract_landmarks(image_path):\n",
    "    \"\"\"Extract hand landmarks from an image.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "    \n",
    "    # If no hands are detected, return None\n",
    "    if not results.multi_hand_landmarks:\n",
    "        return None\n",
    "\n",
    "    landmarks = []\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        for i in range(len(hand_landmarks.landmark)):\n",
    "            x = hand_landmarks.landmark[i].x\n",
    "            y = hand_landmarks.landmark[i].y\n",
    "            # z = hand_landmarks.landmark[i].z\n",
    "            # landmarks.append([x, y, z])  # Store x, y coordinates of each landmark\n",
    "            landmarks.append([x, y])  # Store x, y coordinates of each landmark\n",
    "    \n",
    "    return landmarks\n",
    "\n",
    "def collect_data():\n",
    "    \"\"\"Collect hand gesture data from dataset folder.\"\"\"\n",
    "    data = []  # This will store the landmark data\n",
    "    labels = []  # This will store the corresponding labels (class)\n",
    "\n",
    "    # Loop through each class folder (e.g., class_0, class_1, etc.)\n",
    "    for dir_ in os.listdir(DATA_DIR):\n",
    "        class_folder = os.path.join(DATA_DIR, dir_)\n",
    "        if not os.path.isdir(class_folder):\n",
    "            continue\n",
    "\n",
    "        # Loop through all images in the class folder\n",
    "        for img_path in os.listdir(class_folder):\n",
    "            if img_path.endswith(\".jpg\") or img_path.endswith(\".png\"):\n",
    "                image_path = os.path.join(class_folder, img_path)\n",
    "                \n",
    "                landmarks = extract_landmarks(image_path)\n",
    "                if landmarks:\n",
    "                    data.append(landmarks)  # Add the landmarks data\n",
    "                    labels.append(dir_)  # The folder name is the class label\n",
    "\n",
    "    # Create a dictionary with data and labels\n",
    "    dataset = {\"data\": data, \"labels\": labels}\n",
    "\n",
    "    # Save the dataset to a JSON file\n",
    "    with open(os.path.join(DATA_DIR, \"gesture_data.json\"), 'w') as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "\n",
    "    print(\"Data saved successfully to gesture_data.json\")\n",
    "\n",
    "# Run the data collection\n",
    "collect_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04c9d1",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7280cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {'9eliza': 0, 'one001': 1, 'one002': 2, 'one003': 3, 'palm001': 4, 'palm002': 5}\n",
      "Data cleaned, converted to numerical labels, and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset from gesture_data.json\n",
    "with open(\"dataset/gesture_data.json\", \"r\") as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "# Convert data into NumPy array and reshape\n",
    "data = np.array(data_dict['data'], dtype=np.float32)  # Shape: (samples, 21, 2)\n",
    "num_samples = data.shape[0]\n",
    "\n",
    "# Flatten each sample from (21, 2) to (42,)\n",
    "data = data.reshape(num_samples, -1)  # Shape: (num_samples, 42)\n",
    "\n",
    "# Convert labels into a NumPy array\n",
    "labels = np.array(data_dict['labels'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)  # Now labels are integers: 0, 1, 2, ...\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=0.2, shuffle=True, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Print label mapping\n",
    "print(f\"Label Mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "\n",
    "# Save label mapping for later decoding (optional)\n",
    "with open(\"dataset/label_mapping.json\", \"w\") as f:\n",
    "    json.dump({label: int(idx) for label, idx in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}, f, indent=4)\n",
    "\n",
    "# Save processed dataset\n",
    "split_data = {\n",
    "    \"train\": {\"data\": x_train.tolist(), \"labels\": y_train.tolist()},\n",
    "    \"test\": {\"data\": x_test.tolist(), \"labels\": y_test.tolist()}\n",
    "}\n",
    "\n",
    "with open(\"dataset/train_data.json\", \"w\") as f:\n",
    "    json.dump(split_data['train'], f, indent=4)\n",
    "\n",
    "with open(\"dataset/test_data.json\", \"w\") as f:\n",
    "    json.dump(split_data['test'], f, indent=4)\n",
    "\n",
    "print(\"Data cleaned, converted to numerical labels, and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb53f4",
   "metadata": {},
   "source": [
    "---\n",
    "## Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef102d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf8a09fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully as gesture_classifier.pkl!\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "with open(\"gesture_classifier.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Model saved successfully as gesture_classifier.pkl!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f6e57",
   "metadata": {},
   "source": [
    "### Check Preformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b593b45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = model.predict(x_test)\n",
    "\n",
    "score = accuracy_score(y_predict,y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d12be0",
   "metadata": {},
   "source": [
    "---\n",
    "## Apply model to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfd4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_gesture_label(prediction, label_mapping_path=\"dataset/label_mapping.json\"):\n",
    "    # Load the label mapping\n",
    "    with open(label_mapping_path, \"r\") as f:\n",
    "        label_mapping = json.load(f)\n",
    "    \n",
    "    # Create reverse mapping (number -> name)\n",
    "    reverse_mapping = {v: k for k, v in label_mapping.items()}\n",
    "    \n",
    "    # Handle single predictions or array predictions\n",
    "    if isinstance(prediction, (np.ndarray, list)):\n",
    "        prediction = prediction[0]  # Get first element if array\n",
    "    \n",
    "    return reverse_mapping.get(prediction, \"UNKNOWN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03c85776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"gesture_classifier.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Initialize Mediapipe Hand module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Initialize OpenCV Video Capture (Webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Convert the image to RGB for Mediapipe\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image to detect hands\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmarks in results.multi_hand_landmarks:\n",
    "            # Prepare the landmark points\n",
    "            data_aux = []\n",
    "            for i in range(21):  # 21 landmarks in hand\n",
    "                x = landmarks.landmark[i].x\n",
    "                y = landmarks.landmark[i].y\n",
    "                data_aux.append(x)\n",
    "                data_aux.append(y)\n",
    "\n",
    "            # Convert the data into the format the model expects (flattened)\n",
    "            data_point = np.array(data_aux).reshape(1, -1)\n",
    "\n",
    "            # Make a prediction using the trained model\n",
    "            label = model.predict(data_point)\n",
    "            # Get the prediction probabilities\n",
    "            proba = model.predict_proba(data_point)\n",
    "            # Extract the max probability and the corresponding class\n",
    "            max_proba = np.max(proba)\n",
    "            class_idx = np.argmax(proba)\n",
    "            \n",
    "            # Display the predicted label and the confidence (probability)\n",
    "            cv2.putText(frame, f\"Prediction: {get_gesture_label(label[0])} ({max_proba*100:.2f}%)\", \n",
    "                        (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Draw landmarks and connections\n",
    "            mp.solutions.drawing_utils.draw_landmarks(frame, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Show the frame with predictions\n",
    "    cv2.imshow(\"Hand Gesture Recognition\", frame)\n",
    "\n",
    "    # Break the loop on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3fd67",
   "metadata": {},
   "source": [
    "--- \n",
    "# test connection between ESP32 and my pc using stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f476f5",
   "metadata": {},
   "source": [
    "## I. using webstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e8d27d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to 192.168.128.66:1234...\n",
      "Connected!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import struct\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# ESP32 IP and TCP port\n",
    "ESP32_IP = \"192.168.128.66\"\n",
    "ESP32_PORT = 1234\n",
    "\n",
    "# Frame size (you set this in the ESP32 code)\n",
    "FRAME_WIDTH = 320\n",
    "FRAME_HEIGHT = 240\n",
    "\n",
    "def receive_exact(sock, count):\n",
    "    \"\"\"Receive exact number of bytes from socket.\"\"\"\n",
    "    buf = b\"\"\n",
    "    while len(buf) < count:\n",
    "        chunk = sock.recv(count - len(buf))\n",
    "        if not chunk:\n",
    "            return None\n",
    "        buf += chunk\n",
    "    return buf\n",
    "\n",
    "# Connect to the ESP32 TCP server\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "print(f\"Connecting to {ESP32_IP}:{ESP32_PORT}...\")\n",
    "sock.connect((ESP32_IP, ESP32_PORT))\n",
    "print(\"Connected!\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Read 4-byte length\n",
    "        length_bytes = receive_exact(sock, 4)\n",
    "        if length_bytes is None:\n",
    "            print(\"Disconnected\")\n",
    "            break\n",
    "\n",
    "        frame_size = struct.unpack(\"<I\", length_bytes)[0]\n",
    "\n",
    "        # Read the full frame\n",
    "        frame_data = receive_exact(sock, frame_size)\n",
    "        if frame_data is None:\n",
    "            print(\"Failed to receive full frame.\")\n",
    "            break\n",
    "\n",
    "        # Convert to numpy array (grayscale image)\n",
    "        gray_frame = np.frombuffer(frame_data, dtype=np.uint8).reshape((FRAME_HEIGHT, FRAME_WIDTH))\n",
    "\n",
    "        # Show the image\n",
    "        cv2.imshow(\"ESP32-CAM Stream\", gray_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    sock.close()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3fe81",
   "metadata": {},
   "source": [
    "## II. Using MQTT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df897b0",
   "metadata": {},
   "source": [
    "note is not wroking well is laggi so stream is much better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9e3e58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45042471",
   "metadata": {},
   "source": [
    "---\n",
    "# Connect ESP23-cam with model classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to ESP32-CAM at http://192.168.128.66:81/stream. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"gesture_classifier.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Initialize Mediapipe Hand module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "ip_address = \"192.168.128.66\"\n",
    "esp32_cam_url = f\"http://{ip_address}:81/stream\"\n",
    "\n",
    "# Try to connect to the ESP32 MJPEG stream\n",
    "cap = cv2.VideoCapture(esp32_cam_url)\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Failed to connect to the ESP32-CAM at {esp32_cam_url}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Connected to ESP32-CAM at {esp32_cam_url}. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    frame = cv2.resize(frame, (320, 240))\n",
    "    small_frame = cv2.resize(frame, (320, 240))\n",
    "    img_rgb = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image to detect hands\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmarks in results.multi_hand_landmarks:\n",
    "            # Prepare the landmark points\n",
    "            data_aux = []\n",
    "            for i in range(21):  # 21 landmarks in hand\n",
    "                x = landmarks.landmark[i].x\n",
    "                y = landmarks.landmark[i].y\n",
    "                data_aux.append(x)\n",
    "                data_aux.append(y)\n",
    "\n",
    "            # Convert the data into the format the model expects (flattened)\n",
    "            data_point = np.array(data_aux).reshape(1, -1)\n",
    "\n",
    "            # Make a prediction using the trained model\n",
    "            label = model.predict(data_point)\n",
    "            # Get the prediction probabilities\n",
    "            proba = model.predict_proba(data_point)\n",
    "            # Extract the max probability and the corresponding class\n",
    "            max_proba = np.max(proba)\n",
    "            class_idx = np.argmax(proba)\n",
    "            \n",
    "            # Print the predicted label and the confidence (probability)\n",
    "            print(f\"Prediction: {label[0]} ({max_proba*100:.2f}%)\")\n",
    "\n",
    "    cv2.imshow('ESP32-CAM Stream', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732b4c8",
   "metadata": {},
   "source": [
    "---\n",
    "## time clasulation for mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ac256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe Hand module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Connect to the camera (ESP32 or laptop cam)\n",
    "ip_address = \"192.168.128.66\"\n",
    "esp32_cam_url = f\"http://{ip_address}:81/stream\"\n",
    "cap = cv2.VideoCapture(esp32_cam_url)\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Failed to connect to ESP32-CAM at {esp32_cam_url}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Connected to ESP32-CAM at {esp32_cam_url}. Press 'q' to quit.\")\n",
    "\n",
    "# FPS tracking\n",
    "last_fps_time = time.time()\n",
    "frame_count = 0\n",
    "hand_frame_count = 0\n",
    "no_hand_frame_count = 0\n",
    "\n",
    "while True:\n",
    "    start_capture_time = time.time()\n",
    "    ret, frame = cap.read()\n",
    "    end_capture_time = time.time()\n",
    "\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    frame = cv2.resize(frame, (320, 240))\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    start_mp_time = time.time()\n",
    "    results = hands.process(img_rgb)\n",
    "    end_mp_time = time.time()\n",
    "\n",
    "    # Count if hand is detected or not\n",
    "    frame_count += 1\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_frame_count += 1\n",
    "    else:\n",
    "        no_hand_frame_count += 1\n",
    "\n",
    "    start_show_time = time.time()\n",
    "    cv2.imshow('ESP32-CAM Stream', frame)\n",
    "    end_show_time = time.time()\n",
    "\n",
    "    # Calculate durations\n",
    "    capture_duration = (end_capture_time - start_capture_time) * 1000\n",
    "    mp_duration = (end_mp_time - start_mp_time) * 1000\n",
    "    show_duration = (end_show_time - start_show_time) * 1000\n",
    "\n",
    "    print(f\"Capture: {capture_duration:.2f} ms | MediaPipe: {mp_duration:.2f} ms | Show: {show_duration:.2f} ms\")\n",
    "\n",
    "    # Every second, print FPS and reset counters\n",
    "    current_time = time.time()\n",
    "    if current_time - last_fps_time >= 1.0:\n",
    "        print(f\"\\nFPS: {frame_count} | Hand FPS: {hand_frame_count} | No-Hand FPS: {no_hand_frame_count}\\n\")\n",
    "        frame_count = 0\n",
    "        hand_frame_count = 0\n",
    "        no_hand_frame_count = 0\n",
    "        last_fps_time = current_time\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1028b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import threading\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"gesture_classifier.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Initialize Mediapipe Hand module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "ip_address = \"192.168.128.66\"\n",
    "esp32_cam_url = f\"http://{ip_address}:81/stream\"\n",
    "\n",
    "# Global variables for thread communication\n",
    "latest_prediction = None\n",
    "prediction_lock = threading.Lock()\n",
    "frame_queue = deque(maxlen=1)\n",
    "processing_active = True\n",
    "\n",
    "def process_frames():\n",
    "    global latest_prediction, processing_active\n",
    "    \n",
    "    last_processed_time = datetime.now()\n",
    "    processing_interval = timedelta(milliseconds=200)\n",
    "    \n",
    "    while processing_active:\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        if current_time - last_processed_time >= processing_interval:\n",
    "            # Get the latest frame if available\n",
    "            if len(frame_queue) > 0:\n",
    "                frame = frame_queue[0]\n",
    "                \n",
    "                # Process the frame for hand detection\n",
    "                small_frame = cv2.resize(frame, (320, 240))\n",
    "                img_rgb = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                results = hands.process(img_rgb)\n",
    "                \n",
    "                if results.multi_hand_landmarks:\n",
    "                    for landmarks in results.multi_hand_landmarks:\n",
    "                        # Prepare the landmark points\n",
    "                        data_aux = []\n",
    "                        for i in range(21):  # 21 landmarks in hand\n",
    "                            x = landmarks.landmark[i].x\n",
    "                            y = landmarks.landmark[i].y\n",
    "                            data_aux.append(x)\n",
    "                            data_aux.append(y)\n",
    "\n",
    "                        # Convert the data into the format the model expects\n",
    "                        data_point = np.array(data_aux).reshape(1, -1)\n",
    "\n",
    "                        # Make a prediction using the trained model\n",
    "                        label = model.predict(data_point)\n",
    "                        # Get the prediction probabilities\n",
    "                        proba = model.predict_proba(data_point)\n",
    "                        # Extract the max probability and the corresponding class\n",
    "                        max_proba = np.max(proba)\n",
    "                        class_idx = np.argmax(proba)\n",
    "                        \n",
    "                        # Update the latest prediction\n",
    "                        with prediction_lock:\n",
    "                            latest_prediction = {\n",
    "                                'label': label[0],\n",
    "                                'confidence': max_proba * 100,\n",
    "                                'timestamp': current_time\n",
    "                            }\n",
    "                        \n",
    "                        print(f\"Prediction: {get_gesture_label(label[0])} ({max_proba*100:.2f}%)\")\n",
    "                \n",
    "                last_processed_time = current_time\n",
    "            else:\n",
    "                # No frame available yet, wait a bit\n",
    "                time.sleep(0.01)\n",
    "        else:\n",
    "            # Wait until it's time to process the next frame\n",
    "            time.sleep(0.01)\n",
    "\n",
    "# Start the processing thread\n",
    "processing_thread = threading.Thread(target=process_frames)\n",
    "processing_thread.start()\n",
    "\n",
    "# Try to connect to the ESP32 MJPEG stream\n",
    "cap = cv2.VideoCapture(esp32_cam_url)\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Failed to connect to the ESP32-CAM at {esp32_cam_url}\")\n",
    "    processing_active = False\n",
    "    processing_thread.join()\n",
    "    exit()\n",
    "\n",
    "print(f\"Connected to ESP32-CAM at {esp32_cam_url}. Press 'q' to quit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Resize and display the frame\n",
    "        frame = cv2.resize(frame, (640, 480))\n",
    "        cv2.imshow('ESP32-CAM Stream', frame)\n",
    "        \n",
    "        # Update the frame queue with the latest frame\n",
    "        if len(frame_queue) > 0:\n",
    "            frame_queue.pop()\n",
    "        frame_queue.append(frame.copy())\n",
    "        \n",
    "        # Get the latest prediction if available\n",
    "        with prediction_lock:\n",
    "            if latest_prediction:\n",
    "                # You can use the prediction here if needed\n",
    "                pass\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    # Clean up\n",
    "    processing_active = False\n",
    "    processing_thread.join()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea11976",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Final Code\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc4b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_gesture_label(prediction, label_mapping_path=\"dataset/label_mapping.json\"):\n",
    "    # Load the label mapping\n",
    "    with open(label_mapping_path, \"r\") as f:\n",
    "        label_mapping = json.load(f)\n",
    "    \n",
    "    # Create reverse mapping (number -> name)\n",
    "    reverse_mapping = {v: k for k, v in label_mapping.items()}\n",
    "    \n",
    "    # Handle single predictions or array predictions\n",
    "    if isinstance(prediction, (np.ndarray, list)):\n",
    "        prediction = prediction[0]  # Get first element if array\n",
    "    \n",
    "    return reverse_mapping.get(prediction, \"UNKNOWN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de562d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4244\\1367351767.py:16: DeprecationWarning: Callback API version 1 is deprecated, update to latest version\n",
      "  client = mqtt.Client()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Connecting to MQTT broker...\n",
      "✅ Connected to MQTT broker!\n",
      "✅ Connected to ESP32-CAM at http://192.168.128.66:81/stream. Press 'q' to quit.\n",
      "🤖 Prediction: 5 (60.86%)\n",
      "🤖 Prediction: 3 (97.67%)\n",
      "🤖 Prediction: 3 (63.16%)\n",
      "📤 Publishing majority gesture: 3 (count: 2)\n",
      "🤖 Prediction: 5 (50.75%)\n",
      "🤖 Prediction: 3 (94.91%)\n",
      "🤖 Prediction: 5 (98.99%)\n",
      "📤 Publishing majority gesture: 5 (count: 2)\n",
      "🤖 Prediction: 3 (71.77%)\n",
      "🤖 Prediction: 3 (99.19%)\n",
      "🤖 Prediction: 3 (72.66%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 3 (71.23%)\n",
      "🤖 Prediction: 3 (71.77%)\n",
      "🤖 Prediction: 3 (71.77%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 5 (99.06%)\n",
      "🤖 Prediction: 3 (72.66%)\n",
      "🤖 Prediction: 5 (99.21%)\n",
      "📤 Publishing majority gesture: 5 (count: 2)\n",
      "🤖 Prediction: 5 (99.49%)\n",
      "🤖 Prediction: 5 (99.92%)\n",
      "🤖 Prediction: 5 (99.90%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.91%)\n",
      "🤖 Prediction: 5 (99.52%)\n",
      "🤖 Prediction: 5 (99.47%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.81%)\n",
      "🤖 Prediction: 5 (99.79%)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (99.68%)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (99.68%)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.59%)\n",
      "🤖 Prediction: 5 (99.37%)\n",
      "🤖 Prediction: 5 (99.26%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.80%)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (99.69%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.76%)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (99.85%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (99.64%)\n",
      "🤖 Prediction: 5 (74.34%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 3 (71.15%)\n",
      "🤖 Prediction: 3 (86.74%)\n",
      "🤖 Prediction: 3 (82.25%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 5 (99.02%)\n",
      "🤖 Prediction: 3 (99.09%)\n",
      "🤖 Prediction: 3 (99.36%)\n",
      "📤 Publishing majority gesture: 3 (count: 2)\n",
      "🤖 Prediction: 3 (98.54%)\n",
      "🤖 Prediction: 3 (99.22%)\n",
      "🤖 Prediction: 3 (99.26%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 5 (49.81%)\n",
      "🤖 Prediction: 5 (99.13%)\n",
      "🤖 Prediction: 5 (99.16%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.16%)\n",
      "🤖 Prediction: 5 (99.89%)\n",
      "🤖 Prediction: 5 (99.27%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.16%)\n",
      "🤖 Prediction: 5 (99.16%)\n",
      "🤖 Prediction: 5 (99.16%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.24%)\n",
      "🤖 Prediction: 3 (51.86%)\n",
      "🤖 Prediction: 3 (60.35%)\n",
      "📤 Publishing majority gesture: 3 (count: 2)\n",
      "🤖 Prediction: 3 (83.37%)\n",
      "🤖 Prediction: 5 (99.86%)\n",
      "🤖 Prediction: 3 (84.09%)\n",
      "📤 Publishing majority gesture: 3 (count: 2)\n",
      "🤖 Prediction: 3 (86.21%)\n",
      "🤖 Prediction: 3 (85.06%)\n",
      "🤖 Prediction: 3 (81.31%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 3 (87.02%)\n",
      "🤖 Prediction: 3 (63.30%)\n",
      "🤖 Prediction: 3 (79.69%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 3 (97.63%)\n",
      "🤖 Prediction: 3 (97.76%)\n",
      "🤖 Prediction: 3 (96.95%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 3 (88.84%)\n",
      "🤖 Prediction: 3 (88.94%)\n",
      "🤖 Prediction: 3 (89.00%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 3 (89.08%)\n",
      "🤖 Prediction: 3 (77.72%)\n",
      "🤖 Prediction: 3 (76.99%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 3 (78.01%)\n",
      "🤖 Prediction: 3 (54.63%)\n",
      "🤖 Prediction: 3 (55.62%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 3 (76.97%)\n",
      "🤖 Prediction: 3 (72.60%)\n",
      "🤖 Prediction: 3 (76.41%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 3 (69.06%)\n",
      "🤖 Prediction: 3 (69.69%)\n",
      "🤖 Prediction: 3 (82.13%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 3 (82.22%)\n",
      "🤖 Prediction: 3 (87.70%)\n",
      "🤖 Prediction: 3 (85.23%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 3 (86.80%)\n",
      "🤖 Prediction: 3 (78.03%)\n",
      "🤖 Prediction: 3 (90.02%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 5 (99.57%)\n",
      "🤖 Prediction: 5 (99.36%)\n",
      "🤖 Prediction: 5 (99.25%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.25%)\n",
      "🤖 Prediction: 5 (99.29%)\n",
      "🤖 Prediction: 5 (99.35%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.30%)\n",
      "🤖 Prediction: 5 (99.55%)\n",
      "🤖 Prediction: 5 (99.26%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.26%)\n",
      "🤖 Prediction: 5 (99.26%)\n",
      "🤖 Prediction: 5 (99.70%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.26%)\n",
      "🤖 Prediction: 5 (99.54%)\n",
      "🤖 Prediction: 5 (98.87%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (98.81%)\n",
      "🤖 Prediction: 5 (66.62%)\n",
      "🤖 Prediction: 5 (98.65%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (54.01%)\n",
      "🤖 Prediction: 5 (78.17%)\n",
      "🤖 Prediction: 4 (49.99%)\n",
      "📤 Publishing majority gesture: 5 (count: 2)\n",
      "🤖 Prediction: 4 (55.98%)\n",
      "🤖 Prediction: 5 (52.77%)\n",
      "🤖 Prediction: 5 (99.81%)\n",
      "📤 Publishing majority gesture: 5 (count: 2)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (98.43%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (99.53%)\n",
      "🤖 Prediction: 5 (96.98%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (97.73%)\n",
      "🤖 Prediction: 5 (99.90%)\n",
      "🤖 Prediction: 5 (99.60%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.56%)\n",
      "🤖 Prediction: 5 (99.65%)\n",
      "🤖 Prediction: 5 (99.93%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.66%)\n",
      "🤖 Prediction: 5 (99.90%)\n",
      "🤖 Prediction: 5 (99.96%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.94%)\n",
      "🤖 Prediction: 5 (99.93%)\n",
      "🤖 Prediction: 5 (99.93%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.87%)\n",
      "🤖 Prediction: 5 (99.81%)\n",
      "🤖 Prediction: 5 (99.86%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.88%)\n",
      "🤖 Prediction: 5 (99.53%)\n",
      "🤖 Prediction: 5 (99.81%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.91%)\n",
      "🤖 Prediction: 5 (99.87%)\n",
      "🤖 Prediction: 5 (93.93%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (96.06%)\n",
      "🤖 Prediction: 5 (95.91%)\n",
      "🤖 Prediction: 5 (95.91%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (94.31%)\n",
      "🤖 Prediction: 5 (97.56%)\n",
      "🤖 Prediction: 5 (99.79%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (99.62%)\n",
      "🤖 Prediction: 5 (99.79%)\n",
      "🤖 Prediction: 5 (99.39%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 4 (49.84%)\n",
      "🤖 Prediction: 4 (86.31%)\n",
      "🤖 Prediction: 4 (71.44%)\n",
      "📤 Publishing majority gesture: 4 (count: 3)\n",
      "🤖 Prediction: 4 (51.85%)\n",
      "🤖 Prediction: 5 (99.60%)\n",
      "🤖 Prediction: 5 (97.40%)\n",
      "📤 Publishing majority gesture: 5 (count: 2)\n",
      "🤖 Prediction: 5 (72.23%)\n",
      "🤖 Prediction: 5 (94.24%)\n",
      "🤖 Prediction: 5 (83.34%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 5 (73.21%)\n",
      "🤖 Prediction: 5 (89.05%)\n",
      "🤖 Prediction: 5 (77.21%)\n",
      "📤 Publishing majority gesture: 5 (count: 3)\n",
      "🤖 Prediction: 0 (72.40%)\n",
      "🤖 Prediction: 0 (77.53%)\n",
      "🤖 Prediction: 2 (61.81%)\n",
      "📤 Publishing majority gesture: 0 (count: 2)\n",
      "🤖 Prediction: 2 (60.85%)\n",
      "🤖 Prediction: 3 (80.18%)\n",
      "🤖 Prediction: 2 (55.78%)\n",
      "📤 Publishing majority gesture: 2 (count: 2)\n",
      "🤖 Prediction: 2 (98.30%)\n",
      "🤖 Prediction: 2 (99.56%)\n",
      "🤖 Prediction: 2 (99.52%)\n",
      "📤 Publishing majority gesture: 2 (count: 3)\n",
      "🤖 Prediction: 2 (99.47%)\n",
      "🤖 Prediction: 2 (99.55%)\n",
      "🤖 Prediction: 2 (98.38%)\n",
      "📤 Publishing majority gesture: 2 (count: 3)\n",
      "🤖 Prediction: 2 (98.27%)\n",
      "🤖 Prediction: 2 (99.28%)\n",
      "🤖 Prediction: 1 (77.41%)\n",
      "📤 Publishing majority gesture: 2 (count: 2)\n",
      "🤖 Prediction: 1 (99.27%)\n",
      "🤖 Prediction: 1 (93.52%)\n",
      "🤖 Prediction: 1 (97.24%)\n",
      "📤 Publishing majority gesture: 1 (count: 3)\n",
      "🤖 Prediction: 1 (96.30%)\n",
      "🤖 Prediction: 1 (98.21%)\n",
      "🤖 Prediction: 2 (98.68%)\n",
      "📤 Publishing majority gesture: 1 (count: 2)\n",
      "🤖 Prediction: 2 (99.18%)\n",
      "🤖 Prediction: 2 (99.15%)\n",
      "🤖 Prediction: 2 (99.28%)\n",
      "📤 Publishing majority gesture: 2 (count: 3)\n",
      "🤖 Prediction: 2 (98.96%)\n",
      "🤖 Prediction: 2 (99.32%)\n",
      "🤖 Prediction: 2 (99.32%)\n",
      "📤 Publishing majority gesture: 2 (count: 3)\n",
      "🤖 Prediction: 2 (99.29%)\n",
      "🤖 Prediction: 2 (97.65%)\n",
      "🤖 Prediction: 3 (47.02%)\n",
      "📤 Publishing majority gesture: 2 (count: 2)\n",
      "🤖 Prediction: 1 (82.54%)\n",
      "🤖 Prediction: 3 (96.19%)\n",
      "🤖 Prediction: 3 (98.80%)\n",
      "📤 Publishing majority gesture: 3 (count: 2)\n",
      "🤖 Prediction: 3 (99.91%)\n",
      "🤖 Prediction: 3 (99.68%)\n",
      "🤖 Prediction: 3 (99.29%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 3 (94.54%)\n",
      "🤖 Prediction: 3 (92.82%)\n",
      "🤖 Prediction: 3 (99.06%)\n",
      "📤 Publishing majority gesture: 3 (count: 3)\n",
      "🤖 Prediction: 3 (99.74%)\n",
      "🤖 Prediction: 0 (81.71%)\n",
      "🤖 Prediction: 5 (93.80%)\n",
      "📤 Publishing majority gesture: 3 (count: 1)\n",
      "🤖 Prediction: 4 (42.84%)\n",
      "🤖 Prediction: 0 (43.29%)\n",
      "🤖 Prediction: 0 (43.19%)\n",
      "📤 Publishing majority gesture: 0 (count: 2)\n",
      "🤖 Prediction: 0 (42.96%)\n",
      "🤖 Prediction: 4 (35.52%)\n",
      "🤖 Prediction: 5 (35.03%)\n",
      "📤 Publishing majority gesture: 0 (count: 1)\n",
      "🤖 Prediction: 5 (41.19%)\n",
      "🤖 Prediction: 1 (98.43%)\n",
      "🤖 Prediction: 1 (59.38%)\n",
      "📤 Publishing majority gesture: 1 (count: 2)\n",
      "📴 Disconnected from MQTT broker.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import threading\n",
    "import time\n",
    "from collections import deque, Counter\n",
    "from datetime import datetime, timedelta\n",
    "import paho.mqtt.client as mqtt\n",
    "\n",
    "# ==== MQTT Broker Config ====\n",
    "BROKER_IP = \"192.168.128.88\"\n",
    "MQTT_TOPIC = \"action\"\n",
    "\n",
    "# ==== MQTT Setup ====\n",
    "client = mqtt.Client()\n",
    "def on_connect(client, userdata, flags, rc):\n",
    "    if rc == 0:\n",
    "        print(\"✅ Connected to MQTT broker!\")\n",
    "    else:\n",
    "        print(f\"❌ Connection failed with code {rc}\")\n",
    "client.on_connect = on_connect\n",
    "\n",
    "print(\"🔄 Connecting to MQTT broker...\")\n",
    "client.connect(BROKER_IP, 1883, 60)\n",
    "client.loop_start()\n",
    "time.sleep(1)\n",
    "\n",
    "# ==== Load the model ====\n",
    "model_path = \"gesture_classifier.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# ==== Initialize Mediapipe Hand module ====\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# ==== Camera setup ====\n",
    "ip_address = \"192.168.128.66\"\n",
    "esp32_cam_url = f\"http://{ip_address}:81/stream\"\n",
    "\n",
    "# ==== Globals ====\n",
    "latest_prediction = None\n",
    "prediction_lock = threading.Lock()\n",
    "frame_queue = deque(maxlen=1)\n",
    "processing_active = True\n",
    "prediction_buffer = []\n",
    "\n",
    "# ==== Helper: Publish most frequent label every 3 ====\n",
    "def publish_majority_prediction():\n",
    "    global prediction_buffer\n",
    "    if len(prediction_buffer) >= 3:\n",
    "        counter = Counter(prediction_buffer)\n",
    "        majority_label, count = counter.most_common(1)[0]\n",
    "        print(f\"📤 Publishing majority gesture: {majority_label} (count: {count})\")\n",
    "        client.publish(MQTT_TOPIC, str(get_gesture_label(majority_label)))  # FIXED: convert to str\n",
    "        prediction_buffer = []  # Reset buffer\n",
    "\n",
    "# ==== Thread: Frame processing and prediction ====\n",
    "def process_frames():\n",
    "    global latest_prediction, processing_active, prediction_buffer\n",
    "    last_processed_time = datetime.now()\n",
    "    processing_interval = timedelta(milliseconds=200)\n",
    "\n",
    "    while processing_active:\n",
    "        current_time = datetime.now()\n",
    "        if current_time - last_processed_time >= processing_interval:\n",
    "            if len(frame_queue) > 0:\n",
    "                frame = frame_queue[0]\n",
    "                small_frame = cv2.resize(frame, (320, 240))\n",
    "                img_rgb = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "                results = hands.process(img_rgb)\n",
    "\n",
    "                if results.multi_hand_landmarks:\n",
    "                    for landmarks in results.multi_hand_landmarks:\n",
    "                        data_aux = []\n",
    "                        for i in range(21):\n",
    "                            x = landmarks.landmark[i].x\n",
    "                            y = landmarks.landmark[i].y\n",
    "                            data_aux.append(x)\n",
    "                            data_aux.append(y)\n",
    "\n",
    "                        data_point = np.array(data_aux).reshape(1, -1)\n",
    "                        label = model.predict(data_point)\n",
    "                        proba = model.predict_proba(data_point)\n",
    "                        max_proba = np.max(proba)\n",
    "\n",
    "                        with prediction_lock:\n",
    "                            latest_prediction = {\n",
    "                                'label': label[0],\n",
    "                                'confidence': max_proba * 100,\n",
    "                                'timestamp': current_time\n",
    "                            }\n",
    "\n",
    "                        print(f\"🤖 Prediction: {label[0]} ({max_proba*100:.2f}%)\")\n",
    "                        prediction_buffer.append(label[0])\n",
    "                        publish_majority_prediction()\n",
    "\n",
    "                last_processed_time = current_time\n",
    "            else:\n",
    "                time.sleep(0.01)\n",
    "        else:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "# ==== Start processing thread ====\n",
    "processing_thread = threading.Thread(target=process_frames)\n",
    "processing_thread.start()\n",
    "\n",
    "# ==== Connect to camera stream ====\n",
    "cap = cv2.VideoCapture(esp32_cam_url)\n",
    "if not cap.isOpened():\n",
    "    print(f\"Failed to connect to the ESP32-CAM at {esp32_cam_url}\")\n",
    "    processing_active = False\n",
    "    processing_thread.join()\n",
    "    exit()\n",
    "\n",
    "print(f\"✅ Connected to ESP32-CAM at {esp32_cam_url}. Press 'q' to quit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        frame = cv2.resize(frame, (640, 480))\n",
    "        cv2.imshow('ESP32-CAM Stream', frame)\n",
    "\n",
    "        if len(frame_queue) > 0:\n",
    "            frame_queue.pop()\n",
    "        frame_queue.append(frame.copy())\n",
    "\n",
    "        with prediction_lock:\n",
    "            if latest_prediction:\n",
    "                # Optionally do something with it\n",
    "                pass\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    processing_active = False\n",
    "    processing_thread.join()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    client.loop_stop()\n",
    "    client.disconnect()\n",
    "    print(\"📴 Disconnected from MQTT broker.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f2c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
