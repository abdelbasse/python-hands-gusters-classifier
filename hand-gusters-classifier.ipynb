{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fd0709-ab30-4821-9bc1-b575a2bd4a7a",
   "metadata": {},
   "source": [
    "# Installing OpenCV and Python Dependencies\n",
    "\n",
    "## Install OpenCV\n",
    "To install OpenCV using pip, run the following command:\n",
    "```bash\n",
    "pip install opencv-python\n",
    "```\n",
    "\n",
    "If you need OpenCV with additional functionalities like `opencv-contrib-python`, install:\n",
    "```bash\n",
    "pip install opencv-contrib-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ca1161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    }
   ],
   "source": [
    "print(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d67a082-1c8a-4c11-9f07-265f9c254075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "if not capture.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = capture.read()  # Capture frame-by-frame\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('Camera Feed', frame)  # Display the frame\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84703fea-d457-4a49-a687-22b100df173a",
   "metadata": {},
   "source": [
    "## Install MediaPipe\n",
    "MediaPipe is required for hand tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c777c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ffc73d-0d4c-4eb2-8f7e-bbe58bc0a08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (3.10.1)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (4.25.6)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from jax->mediapipe) (1.15.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\@jupyter_projects\\@hackathon+competition\\competitions\\hand geaster reader - mediapipe\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python3.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62b4102a-df1c-40ff-ac9e-40556f69ac4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drawing is done: Distance = 0.158\n",
      "Drawing is done: Distance = 0.177\n",
      "Drawing is done: Distance = 0.238\n",
      "Drawing is done: Distance = 0.238\n",
      "Drawing is done: Distance = 0.191\n",
      "Drawing is done: Distance = 0.123\n",
      "Drawing is done: Distance = 0.076\n",
      "Drawing is done: Distance = 0.141\n",
      "Drawing is done: Distance = 0.226\n",
      "Drawing is done: Distance = 0.287\n",
      "Drawing is done: Distance = 0.230\n",
      "Drawing is done: Distance = 0.100\n",
      "Drawing is done: Distance = 0.114\n",
      "Drawing is done: Distance = 0.278\n",
      "Drawing is done: Distance = 0.293\n",
      "Drawing is done: Distance = 0.296\n",
      "Drawing is done: Distance = 0.108\n",
      "Drawing is done: Distance = 0.068\n",
      "Drawing is done: Distance = 0.042\n",
      "Drawing is done: Distance = 0.136\n",
      "Drawing is done: Distance = 0.170\n",
      "Drawing is done: Distance = 0.258\n",
      "Drawing is done: Distance = 0.269\n",
      "Drawing is done: Distance = 0.327\n",
      "Drawing is done: Distance = 0.340\n",
      "Drawing is done: Distance = 0.376\n",
      "Drawing is done: Distance = 0.395\n",
      "Drawing is done: Distance = 0.396\n",
      "Drawing is done: Distance = 0.375\n",
      "Drawing is done: Distance = 0.305\n",
      "Drawing is done: Distance = 0.174\n",
      "Drawing is done: Distance = 0.144\n",
      "Drawing is done: Distance = 0.093\n",
      "Drawing is done: Distance = 0.080\n",
      "Drawing is done: Distance = 0.066\n",
      "Drawing is done: Distance = 0.184\n",
      "Drawing is done: Distance = 0.223\n",
      "Drawing is done: Distance = 0.261\n",
      "Drawing is done: Distance = 0.367\n",
      "Drawing is done: Distance = 0.387\n",
      "Drawing is done: Distance = 0.429\n",
      "Drawing is done: Distance = 0.428\n",
      "Drawing is done: Distance = 0.418\n",
      "Drawing is done: Distance = 0.408\n",
      "Drawing is done: Distance = 0.325\n",
      "Drawing is done: Distance = 0.207\n",
      "Drawing is done: Distance = 0.153\n",
      "Drawing is done: Distance = 0.077\n",
      "Drawing is done: Distance = 0.051\n",
      "Drawing is done: Distance = 0.048\n",
      "Drawing is done: Distance = 0.041\n",
      "Drawing is done: Distance = 0.045\n",
      "Drawing is done: Distance = 0.139\n",
      "Drawing is done: Distance = 0.148\n",
      "Drawing is done: Distance = 0.133\n",
      "Drawing is done: Distance = 0.051\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "def drawLine_landmark(frame, hand_landmarks):\n",
    "    index_finger_tip = hand_landmarks.landmark[8]\n",
    "    thumb_tip = hand_landmarks.landmark[4]\n",
    "\n",
    "    # Convert normalized coordinates (0-1) to pixel values\n",
    "    index_finger_x = int(index_finger_tip.x * frame.shape[1])\n",
    "    index_finger_y = int(index_finger_tip.y * frame.shape[0])\n",
    "    thumb_x = int(thumb_tip.x * frame.shape[1])\n",
    "    thumb_y = int(thumb_tip.y * frame.shape[0])\n",
    "\n",
    "    # Calculate Euclidean distance\n",
    "    dis = ((index_finger_tip.x - thumb_tip.x)**2 + (index_finger_tip.y - thumb_tip.y)**2)**0.5\n",
    "\n",
    "    # Calculate midpoint of the line\n",
    "    mid_x = (index_finger_x + thumb_x) // 2\n",
    "    mid_y = (index_finger_y + thumb_y) // 2\n",
    "\n",
    "    # Draw line\n",
    "    cv2.line(frame, (index_finger_x, index_finger_y), (thumb_x, thumb_y), (205, 55, 120), 5)\n",
    "\n",
    "    # Draw text at the midpoint\n",
    "    text = f\"{dis:.3f}\"\n",
    "    font_scale = 0.6\n",
    "    font_thickness = 2\n",
    "    text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)[0]\n",
    "    text_x = mid_x - text_size[0] // 2  # Center text horizontally\n",
    "    text_y = mid_y + text_size[1] // 2  # Center text vertically\n",
    "    cv2.putText(frame, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness, cv2.LINE_AA)\n",
    "\n",
    "    depth = index_finger_tip.z  # Get Z-value (depth)\n",
    "    text_Des = f\"Depth: {depth:.3f}\"  # Format to 3 decimal places\n",
    "    cv2.putText(frame, text_Des, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    print(f\"Drawing is done: Distance = {dis:.3f}\")\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Open the camera\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "if not capture.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        heightFrame, widthFrame, _ = frame.shape\n",
    "\n",
    "        # Convert image to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = hands.process(rgb_frame)\n",
    "        \n",
    "        if result.multi_hand_landmarks:\n",
    "            for hand_landmarks, handedness in zip(result.multi_hand_landmarks, result.multi_handedness):\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                # Get hand label (Left or Right)\n",
    "                hand_label = handedness.classification[0].label\n",
    "                x, y = int(hand_landmarks.landmark[0].x * widthFrame), int(hand_landmarks.landmark[0].y * heightFrame)\n",
    "                cv2.putText(frame, f\"{hand_label} Hand\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "\n",
    "                # Draw line between index finger and thumb\n",
    "                drawLine_landmark(frame, hand_landmarks)\n",
    "\n",
    "        # Create a semi-transparent overlay for the red circle\n",
    "        overlay = frame.copy()\n",
    "        cv2.circle(overlay, (widthFrame // 2, heightFrame // 2), 75, (0, 0, 255), -1)  # Red filled circle\n",
    "        alpha = 0.5  # Transparency level\n",
    "        cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "\n",
    "        cv2.imshow('Hand Tracking', frame)  # Display the frame\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1329a82",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# I. Interface hands gusters\n",
    "make a class or interface that will take hands gusters and assign to each on of them a fucntion and then we can programe these fucntion as we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb42e1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 's' to start recording, and 'q' to quit.\n",
      "Starting recording...\n",
      "Recording finished.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class HandGesture:\n",
    "    def __init__(self, name, landmarks=None):\n",
    "        self.name = name\n",
    "        self.landmarks = landmarks if landmarks else []\n",
    "    \n",
    "    def set_landmarks(self, landmarks):\n",
    "        self.landmarks = landmarks\n",
    "    \n",
    "    def compare_gesture(self, detected_landmarks):\n",
    "        \"\"\"Compare the detected hand with the stored gesture.\"\"\"\n",
    "        if not self.landmarks or not detected_landmarks:\n",
    "            return False\n",
    "        \n",
    "        # Compute Euclidean distance between corresponding landmarks\n",
    "        distances = [np.linalg.norm(\n",
    "            np.array([self.landmarks[i].x, self.landmarks[i].y]) -\n",
    "            np.array([detected_landmarks[i].x, detected_landmarks[i].y])\n",
    "        ) for i in range(len(self.landmarks))]\n",
    "        \n",
    "        return np.mean(distances) < 0.05  # Adjust threshold as needed\n",
    "\n",
    "class HandDetector:\n",
    "    def __init__(self):\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.hands = self.mp_hands.Hands(static_image_mode=True, max_num_hands=1)\n",
    "    \n",
    "    def detect_hands(self, image):\n",
    "        results = self.hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        if not results.multi_hand_landmarks:\n",
    "            return None  # No hands detected\n",
    "        \n",
    "        return results.multi_hand_landmarks[0].landmark\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load the progress from the JSON file, or initialize if not found.\"\"\"\n",
    "    progress_file = 'progress.json'\n",
    "    \n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        # If no progress file, initialize it with the first class index\n",
    "        return {'last_class_index': 0}\n",
    "\n",
    "def save_progress(progress):\n",
    "    \"\"\"Save the current progress to the JSON file.\"\"\"\n",
    "    with open('progress.json', 'w') as f:\n",
    "        json.dump(progress, f)\n",
    "\n",
    "def create_class_folder(class_index):\n",
    "    dataset_dir = 'dataset'\n",
    "    class_folder = os.path.join(dataset_dir, f'class_{class_index}')\n",
    "    \n",
    "    # Check if class folder already exists, and if so, increment the class_index\n",
    "    while os.path.exists(class_folder):\n",
    "        class_index += 1\n",
    "        class_folder = os.path.join(dataset_dir, f'class_{class_index}')\n",
    "    \n",
    "    # Create the class folder\n",
    "    os.makedirs(class_folder)\n",
    "    return class_folder, class_index\n",
    "\n",
    "def capture_and_record(nbr_sample_per_gestures = 100):\n",
    "    progress = load_progress()  # Load the current progress\n",
    "    class_index = progress['last_class_index']  # Get the last used class index\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    detector = HandDetector()\n",
    "\n",
    "    class_folder, class_index = create_class_folder(class_index)\n",
    "    progress['last_class_index'] = class_index  # Update the progress\n",
    "    save_progress(progress)  # Save progress to JSON file\n",
    "    \n",
    "    frame_count = 0\n",
    "    num_frames = nbr_sample_per_gestures\n",
    "    recording = False\n",
    "    \n",
    "    print(\"Press 's' to start recording, and 'q' to quit.\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Create a copy of the frame for saving (without the text)\n",
    "        frame_to_save = frame.copy()\n",
    "        \n",
    "        # Detect hand landmarks\n",
    "        landmarks = detector.detect_hands(frame)\n",
    "        \n",
    "        if landmarks:\n",
    "            # Display recording status on the original frame (with text)\n",
    "            if recording:\n",
    "                cv2.putText(frame, \"Recording...\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                cv2.putText(frame, f\"Frames recorded: {frame_count}/{num_frames}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                \n",
    "                # Save the frame after every successful capture (without the text)\n",
    "                if frame_count < num_frames:\n",
    "                    frame_name = f\"{frame_count + 1}.jpg\"\n",
    "                    cv2.imwrite(os.path.join(class_folder, frame_name), frame_to_save)  # Save the frame without text\n",
    "                    frame_count += 1\n",
    "                \n",
    "                if frame_count >= num_frames:\n",
    "                    print(\"Recording finished.\")\n",
    "                    cv2.putText(frame, \"Recording Finished\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                    recording = False\n",
    "                    class_folder, class_index = create_class_folder(class_index)\n",
    "                    progress['last_class_index'] = class_index  # Update the progress\n",
    "                    save_progress(progress) \n",
    "            else:\n",
    "                # Show \"Press 's' to start\" message on the original frame\n",
    "                cv2.putText(frame, \"Press 's' to start recording\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(\"Hand Gesture Recorder\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('s'):  # Start recording when 's' is pressed\n",
    "            if not recording:\n",
    "                print(\"Starting recording...\")\n",
    "                recording = True\n",
    "                frame_count = 0  # Reset frame count\n",
    "        elif key == ord('q'):  # Quit when 'q' is pressed\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "capture_and_record(200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a731bbb",
   "metadata": {},
   "source": [
    "---\n",
    "## Proccess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6053b869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to gesture_data.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "\n",
    "DATA_DIR = 'dataset'  # Define the dataset directory\n",
    "\n",
    "# Initialize hand detector using MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1)\n",
    "\n",
    "def extract_landmarks(image_path):\n",
    "    \"\"\"Extract hand landmarks from an image.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "    \n",
    "    # If no hands are detected, return None\n",
    "    if not results.multi_hand_landmarks:\n",
    "        return None\n",
    "\n",
    "    landmarks = []\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        for i in range(len(hand_landmarks.landmark)):\n",
    "            x = hand_landmarks.landmark[i].x\n",
    "            y = hand_landmarks.landmark[i].y\n",
    "            # z = hand_landmarks.landmark[i].z\n",
    "            # landmarks.append([x, y, z])  # Store x, y coordinates of each landmark\n",
    "            landmarks.append([x, y])  # Store x, y coordinates of each landmark\n",
    "    \n",
    "    return landmarks\n",
    "\n",
    "def collect_data():\n",
    "    \"\"\"Collect hand gesture data from dataset folder.\"\"\"\n",
    "    data = []  # This will store the landmark data\n",
    "    labels = []  # This will store the corresponding labels (class)\n",
    "\n",
    "    # Loop through each class folder (e.g., class_0, class_1, etc.)\n",
    "    for dir_ in os.listdir(DATA_DIR):\n",
    "        class_folder = os.path.join(DATA_DIR, dir_)\n",
    "        if not os.path.isdir(class_folder):\n",
    "            continue\n",
    "\n",
    "        # Loop through all images in the class folder\n",
    "        for img_path in os.listdir(class_folder):\n",
    "            if img_path.endswith(\".jpg\") or img_path.endswith(\".png\"):\n",
    "                image_path = os.path.join(class_folder, img_path)\n",
    "                \n",
    "                landmarks = extract_landmarks(image_path)\n",
    "                if landmarks:\n",
    "                    data.append(landmarks)  # Add the landmarks data\n",
    "                    labels.append(dir_)  # The folder name is the class label\n",
    "\n",
    "    # Create a dictionary with data and labels\n",
    "    dataset = {\"data\": data, \"labels\": labels}\n",
    "\n",
    "    # Save the dataset to a JSON file\n",
    "    with open(os.path.join(DATA_DIR, \"gesture_data.json\"), 'w') as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "\n",
    "    print(\"Data saved successfully to gesture_data.json\")\n",
    "\n",
    "# Run the data collection\n",
    "collect_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04c9d1",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7280cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {'class_0': 0, 'class_1': 1, 'class_2': 2, 'class_3': 3}\n",
      "Data cleaned, converted to numerical labels, and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset from gesture_data.json\n",
    "with open(\"dataset/gesture_data.json\", \"r\") as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "# Convert data into NumPy array and reshape\n",
    "data = np.array(data_dict['data'], dtype=np.float32)  # Shape: (samples, 21, 2)\n",
    "num_samples = data.shape[0]\n",
    "\n",
    "# Flatten each sample from (21, 2) to (42,)\n",
    "data = data.reshape(num_samples, -1)  # Shape: (num_samples, 42)\n",
    "\n",
    "# Convert labels into a NumPy array\n",
    "labels = np.array(data_dict['labels'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)  # Now labels are integers: 0, 1, 2, ...\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=0.2, shuffle=True, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Print label mapping\n",
    "print(f\"Label Mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "\n",
    "# Save label mapping for later decoding (optional)\n",
    "with open(\"dataset/label_mapping.json\", \"w\") as f:\n",
    "    json.dump({label: int(idx) for label, idx in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}, f, indent=4)\n",
    "\n",
    "# Save processed dataset\n",
    "split_data = {\n",
    "    \"train\": {\"data\": x_train.tolist(), \"labels\": y_train.tolist()},\n",
    "    \"test\": {\"data\": x_test.tolist(), \"labels\": y_test.tolist()}\n",
    "}\n",
    "\n",
    "with open(\"dataset/train_data.json\", \"w\") as f:\n",
    "    json.dump(split_data['train'], f, indent=4)\n",
    "\n",
    "with open(\"dataset/test_data.json\", \"w\") as f:\n",
    "    json.dump(split_data['test'], f, indent=4)\n",
    "\n",
    "print(\"Data cleaned, converted to numerical labels, and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb53f4",
   "metadata": {},
   "source": [
    "---\n",
    "## Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef102d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a09fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully as gesture_classifier.pkl!\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "with open(\"gesture_classifier.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Model saved successfully as gesture_classifier.pkl!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f6e57",
   "metadata": {},
   "source": [
    "### Check Preformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b593b45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = model.predict(x_test)\n",
    "\n",
    "score = accuracy_score(y_predict,y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d12be0",
   "metadata": {},
   "source": [
    "---\n",
    "## Apply model to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03c85776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"gesture_classifier.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Initialize Mediapipe Hand module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Initialize OpenCV Video Capture (Webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Convert the image to RGB for Mediapipe\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image to detect hands\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmarks in results.multi_hand_landmarks:\n",
    "            # Prepare the landmark points\n",
    "            data_aux = []\n",
    "            for i in range(21):  # 21 landmarks in hand\n",
    "                x = landmarks.landmark[i].x\n",
    "                y = landmarks.landmark[i].y\n",
    "                data_aux.append(x)\n",
    "                data_aux.append(y)\n",
    "\n",
    "            # Convert the data into the format the model expects (flattened)\n",
    "            data_point = np.array(data_aux).reshape(1, -1)\n",
    "\n",
    "            # Make a prediction using the trained model\n",
    "            label = model.predict(data_point)\n",
    "            # Get the prediction probabilities\n",
    "            proba = model.predict_proba(data_point)\n",
    "            # Extract the max probability and the corresponding class\n",
    "            max_proba = np.max(proba)\n",
    "            class_idx = np.argmax(proba)\n",
    "            \n",
    "            # Display the predicted label and the confidence (probability)\n",
    "            cv2.putText(frame, f\"Prediction: {label[0]} ({max_proba*100:.2f}%)\", \n",
    "                        (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Draw landmarks and connections\n",
    "            mp.solutions.drawing_utils.draw_landmarks(frame, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Show the frame with predictions\n",
    "    cv2.imshow(\"Hand Gesture Recognition\", frame)\n",
    "\n",
    "    # Break the loop on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d27d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
