{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69b592c",
   "metadata": {},
   "source": [
    "# Installing OpenCV and Python Dependencies\n",
    "\n",
    "## Install OpenCV\n",
    "To install OpenCV using pip, run the following command:\n",
    "```bash\n",
    "pip install opencv-python\n",
    "```\n",
    "\n",
    "If you need OpenCV with additional functionalities like `opencv-contrib-python`, install:\n",
    "```bash\n",
    "pip install opencv-contrib-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac280d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyhton3 --version #it should be between 10 and 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd79fd",
   "metadata": {},
   "source": [
    "## Install MediaPipe\n",
    "MediaPipe is required for hand tracking:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300efce",
   "metadata": {},
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67fb26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d03f082",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# I. Interface Save hands gusters :\n",
    "make a class or interface that will take hands gusters and assign to each on of them a fucntion and then we can programe these fucntion as we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc6f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class HandGesture:\n",
    "    def __init__(self, name, landmarks=None):\n",
    "        self.name = name\n",
    "        self.landmarks = landmarks if landmarks else []\n",
    "    \n",
    "    def set_landmarks(self, landmarks):\n",
    "        self.landmarks = landmarks\n",
    "    \n",
    "    def compare_gesture(self, detected_landmarks):\n",
    "        \"\"\"Compare the detected hand with the stored gesture.\"\"\"\n",
    "        if not self.landmarks or not detected_landmarks:\n",
    "            return False\n",
    "        \n",
    "        # Compute Euclidean distance between corresponding landmarks\n",
    "        distances = [np.linalg.norm(\n",
    "            np.array([self.landmarks[i].x, self.landmarks[i].y]) -\n",
    "            np.array([detected_landmarks[i].x, detected_landmarks[i].y])\n",
    "        ) for i in range(len(self.landmarks))]\n",
    "        \n",
    "        return np.mean(distances) < 0.05  # Adjust threshold as needed\n",
    "\n",
    "class HandDetector:\n",
    "    def __init__(self):\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.hands = self.mp_hands.Hands(static_image_mode=True, max_num_hands=1)\n",
    "    \n",
    "    def detect_hands(self, image):\n",
    "        results = self.hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        if not results.multi_hand_landmarks:\n",
    "            return None  # No hands detected\n",
    "        \n",
    "        return results.multi_hand_landmarks[0].landmark\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load the progress from the JSON file, or initialize if not found.\"\"\"\n",
    "    progress_file = 'progress.json'\n",
    "    \n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        # If no progress file, initialize it with the first class index\n",
    "        return {'last_class_index': 0}\n",
    "\n",
    "def save_progress(progress):\n",
    "    \"\"\"Save the current progress to the JSON file.\"\"\"\n",
    "    with open('progress.json', 'w') as f:\n",
    "        json.dump(progress, f)\n",
    "\n",
    "def create_class_folder(class_index):\n",
    "    dataset_dir = 'dataset'\n",
    "    class_folder = os.path.join(dataset_dir, f'class_{class_index}')\n",
    "    \n",
    "    # Check if class folder already exists, and if so, increment the class_index\n",
    "    while os.path.exists(class_folder):\n",
    "        class_index += 1\n",
    "        class_folder = os.path.join(dataset_dir, f'class_{class_index}')\n",
    "    \n",
    "    # Create the class folder\n",
    "    os.makedirs(class_folder)\n",
    "    return class_folder, class_index\n",
    "\n",
    "def capture_and_record(nbr_sample_per_gestures = 100):\n",
    "    progress = load_progress()  # Load the current progress\n",
    "    class_index = progress['last_class_index']  # Get the last used class index\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    detector = HandDetector()\n",
    "\n",
    "    class_folder, class_index = create_class_folder(class_index)\n",
    "    progress['last_class_index'] = class_index  # Update the progress\n",
    "    save_progress(progress)  # Save progress to JSON file\n",
    "    \n",
    "    frame_count = 0\n",
    "    num_frames = nbr_sample_per_gestures\n",
    "    recording = False\n",
    "    \n",
    "    print(\"Press 's' to start recording, and 'q' to quit.\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Create a copy of the frame for saving (without the text)\n",
    "        frame_to_save = frame.copy()\n",
    "        \n",
    "        # Detect hand landmarks\n",
    "        landmarks = detector.detect_hands(frame)\n",
    "        \n",
    "        if landmarks:\n",
    "            # Display recording status on the original frame (with text)\n",
    "            if recording:\n",
    "                cv2.putText(frame, \"Recording...\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                cv2.putText(frame, f\"Frames recorded: {frame_count}/{num_frames}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                \n",
    "                # Save the frame after every successful capture (without the text)\n",
    "                if frame_count < num_frames:\n",
    "                    frame_name = f\"{frame_count + 1}.jpg\"\n",
    "                    cv2.imwrite(os.path.join(class_folder, frame_name), frame_to_save)  # Save the frame without text\n",
    "                    frame_count += 1\n",
    "                \n",
    "                if frame_count >= num_frames:\n",
    "                    print(\"Recording finished.\")\n",
    "                    cv2.putText(frame, \"Recording Finished\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                    recording = False\n",
    "                    class_folder, class_index = create_class_folder(class_index)\n",
    "                    progress['last_class_index'] = class_index  # Update the progress\n",
    "                    save_progress(progress) \n",
    "            else:\n",
    "                # Show \"Press 's' to start\" message on the original frame\n",
    "                cv2.putText(frame, \"Press 's' to start recording\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(\"Hand Gesture Recorder\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('s'):  # Start recording when 's' is pressed\n",
    "            if not recording:\n",
    "                print(\"Starting recording...\")\n",
    "                recording = True\n",
    "                frame_count = 0  # Reset frame count\n",
    "        elif key == ord('q'):  # Quit when 'q' is pressed\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "capture_and_record(200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61ca8c",
   "metadata": {},
   "source": [
    "---\n",
    "# II. Proccess data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56161148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import json\n",
    "\n",
    "DATA_DIR = 'dataset'  # Define the dataset directory\n",
    "\n",
    "# Initialize hand detector using MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1)\n",
    "\n",
    "def extract_landmarks(image_path):\n",
    "    \"\"\"Extract hand landmarks from an image.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "    \n",
    "    # If no hands are detected, return None\n",
    "    if not results.multi_hand_landmarks:\n",
    "        return None\n",
    "\n",
    "    landmarks = []\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        for i in range(len(hand_landmarks.landmark)):\n",
    "            x = hand_landmarks.landmark[i].x\n",
    "            y = hand_landmarks.landmark[i].y\n",
    "            # z = hand_landmarks.landmark[i].z\n",
    "            # landmarks.append([x, y, z])  # Store x, y coordinates of each landmark\n",
    "            landmarks.append([x, y])  # Store x, y coordinates of each landmark\n",
    "    \n",
    "    return landmarks\n",
    "\n",
    "def collect_data():\n",
    "    \"\"\"Collect hand gesture data from dataset folder.\"\"\"\n",
    "    data = []  # This will store the landmark data\n",
    "    labels = []  # This will store the corresponding labels (class)\n",
    "\n",
    "    # Loop through each class folder (e.g., class_0, class_1, etc.)\n",
    "    for dir_ in os.listdir(DATA_DIR):\n",
    "        class_folder = os.path.join(DATA_DIR, dir_)\n",
    "        if not os.path.isdir(class_folder):\n",
    "            continue\n",
    "\n",
    "        # Loop through all images in the class folder\n",
    "        for img_path in os.listdir(class_folder):\n",
    "            if img_path.endswith(\".jpg\") or img_path.endswith(\".png\"):\n",
    "                image_path = os.path.join(class_folder, img_path)\n",
    "                \n",
    "                landmarks = extract_landmarks(image_path)\n",
    "                if landmarks:\n",
    "                    data.append(landmarks)  # Add the landmarks data\n",
    "                    labels.append(dir_)  # The folder name is the class label\n",
    "\n",
    "    # Create a dictionary with data and labels\n",
    "    dataset = {\"data\": data, \"labels\": labels}\n",
    "\n",
    "    # Save the dataset to a JSON file\n",
    "    with open(os.path.join(DATA_DIR, \"gesture_data.json\"), 'w') as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "\n",
    "    print(\"Data saved successfully to gesture_data.json\")\n",
    "\n",
    "# Run the data collection\n",
    "collect_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c78a9",
   "metadata": {},
   "source": [
    "---\n",
    "# III. Clean & Split data :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb8f384",
   "metadata": {},
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset from gesture_data.json\n",
    "with open(\"dataset/gesture_data.json\", \"r\") as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "# Convert data into NumPy array and reshape\n",
    "data = np.array(data_dict['data'], dtype=np.float32)  # Shape: (samples, 21, 2)\n",
    "num_samples = data.shape[0]\n",
    "\n",
    "# Flatten each sample from (21, 2) to (42,)\n",
    "data = data.reshape(num_samples, -1)  # Shape: (num_samples, 42)\n",
    "\n",
    "# Convert labels into a NumPy array\n",
    "labels = np.array(data_dict['labels'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)  # Now labels are integers: 0, 1, 2, ...\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=0.2, shuffle=True, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Print label mapping\n",
    "print(f\"Label Mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "\n",
    "# Save label mapping for later decoding (optional)\n",
    "with open(\"dataset/label_mapping.json\", \"w\") as f:\n",
    "    json.dump({label: int(idx) for label, idx in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}, f, indent=4)\n",
    "\n",
    "# Save processed dataset\n",
    "split_data = {\n",
    "    \"train\": {\"data\": x_train.tolist(), \"labels\": y_train.tolist()},\n",
    "    \"test\": {\"data\": x_test.tolist(), \"labels\": y_test.tolist()}\n",
    "}\n",
    "\n",
    "with open(\"dataset/train_data.json\", \"w\") as f:\n",
    "    json.dump(split_data['train'], f, indent=4)\n",
    "\n",
    "with open(\"dataset/test_data.json\", \"w\") as f:\n",
    "    json.dump(split_data['test'], f, indent=4)\n",
    "\n",
    "print(\"Data cleaned, converted to numerical labels, and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc6edf",
   "metadata": {},
   "source": [
    "---\n",
    "# IV. Train & Save model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c08e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "with open(\"gesture_classifier.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Model saved successfully as gesture_classifier.pkl!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a20e1b",
   "metadata": {},
   "source": [
    "## 1. Check Preformance :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7408830",
   "metadata": {},
   "source": [
    "y_predict = model.predict(x_test)\n",
    "\n",
    "score = accuracy_score(y_predict,y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a4b46",
   "metadata": {},
   "source": [
    "---\n",
    "# V. Apply model :\n",
    "## 1. TEST :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9397449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_gesture_label(prediction, label_mapping_path=\"dataset/label_mapping.json\"):\n",
    "    # Load the label mapping\n",
    "    with open(label_mapping_path, \"r\") as f:\n",
    "        label_mapping = json.load(f)\n",
    "    \n",
    "    # Create reverse mapping (number -> name)\n",
    "    reverse_mapping = {v: k for k, v in label_mapping.items()}\n",
    "    \n",
    "    # Handle single predictions or array predictions\n",
    "    if isinstance(prediction, (np.ndarray, list)):\n",
    "        prediction = prediction[0]  # Get first element if array\n",
    "    \n",
    "    return reverse_mapping.get(prediction, \"UNKNOWN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"gesture_classifier.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Initialize Mediapipe Hand module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Initialize OpenCV Video Capture (Webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Convert the image to RGB for Mediapipe\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image to detect hands\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmarks in results.multi_hand_landmarks:\n",
    "            # Prepare the landmark points\n",
    "            data_aux = []\n",
    "            for i in range(21):  # 21 landmarks in hand\n",
    "                x = landmarks.landmark[i].x\n",
    "                y = landmarks.landmark[i].y\n",
    "                data_aux.append(x)\n",
    "                data_aux.append(y)\n",
    "\n",
    "            # Convert the data into the format the model expects (flattened)\n",
    "            data_point = np.array(data_aux).reshape(1, -1)\n",
    "\n",
    "            # Make a prediction using the trained model\n",
    "            label = model.predict(data_point)\n",
    "            # Get the prediction probabilities\n",
    "            proba = model.predict_proba(data_point)\n",
    "            # Extract the max probability and the corresponding class\n",
    "            max_proba = np.max(proba)\n",
    "            class_idx = np.argmax(proba)\n",
    "            \n",
    "            # Display the predicted label and the confidence (probability)\n",
    "            cv2.putText(frame, f\"Prediction: {get_gesture_label(label[0])} ({max_proba*100:.2f}%)\", \n",
    "                        (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Draw landmarks and connections\n",
    "            mp.solutions.drawing_utils.draw_landmarks(frame, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Show the frame with predictions\n",
    "    cv2.imshow(\"Hand Gesture Recognition\", frame)\n",
    "\n",
    "    # Break the loop on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0618b523",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 2. Final Code:\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89199eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import threading\n",
    "import time\n",
    "from collections import deque, Counter\n",
    "from datetime import datetime, timedelta\n",
    "import paho.mqtt.client as mqtt\n",
    "\n",
    "# ==== MQTT Broker Config ====\n",
    "BROKER_IP = \"192.168.128.88\"\n",
    "MQTT_TOPIC = \"action\"\n",
    "\n",
    "# ==== MQTT Setup ====\n",
    "client = mqtt.Client()\n",
    "def on_connect(client, userdata, flags, rc):\n",
    "    if rc == 0:\n",
    "        print(\"âœ… Connected to MQTT broker!\")\n",
    "    else:\n",
    "        print(f\"âŒ Connection failed with code {rc}\")\n",
    "client.on_connect = on_connect\n",
    "\n",
    "print(\"ðŸ”„ Connecting to MQTT broker...\")\n",
    "client.connect(BROKER_IP, 1883, 60)\n",
    "client.loop_start()\n",
    "time.sleep(1)\n",
    "\n",
    "# ==== Load the model ====\n",
    "model_path = \"gesture_classifier.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# ==== Initialize Mediapipe Hand module ====\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# ==== Camera setup ====\n",
    "ip_address = \"192.168.128.66\"\n",
    "esp32_cam_url = f\"http://{ip_address}:81/stream\"\n",
    "\n",
    "# ==== Globals ====\n",
    "latest_prediction = None\n",
    "prediction_lock = threading.Lock()\n",
    "frame_queue = deque(maxlen=1)\n",
    "processing_active = True\n",
    "prediction_buffer = []\n",
    "\n",
    "# ==== Helper: Publish most frequent label every 3 ====\n",
    "def publish_majority_prediction():\n",
    "    global prediction_buffer\n",
    "    if len(prediction_buffer) >= 3:\n",
    "        counter = Counter(prediction_buffer)\n",
    "        majority_label, count = counter.most_common(1)[0]\n",
    "        print(f\"ðŸ“¤ Publishing majority gesture: {majority_label} (count: {count})\")\n",
    "        client.publish(MQTT_TOPIC, str(get_gesture_label(majority_label)))  # FIXED: convert to str\n",
    "        prediction_buffer = []  # Reset buffer\n",
    "\n",
    "# ==== Thread: Frame processing and prediction ====\n",
    "def process_frames():\n",
    "    global latest_prediction, processing_active, prediction_buffer\n",
    "    last_processed_time = datetime.now()\n",
    "    processing_interval = timedelta(milliseconds=200)\n",
    "\n",
    "    while processing_active:\n",
    "        current_time = datetime.now()\n",
    "        if current_time - last_processed_time >= processing_interval:\n",
    "            if len(frame_queue) > 0:\n",
    "                frame = frame_queue[0]\n",
    "                small_frame = cv2.resize(frame, (320, 240))\n",
    "                img_rgb = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "                results = hands.process(img_rgb)\n",
    "\n",
    "                if results.multi_hand_landmarks:\n",
    "                    for landmarks in results.multi_hand_landmarks:\n",
    "                        data_aux = []\n",
    "                        for i in range(21):\n",
    "                            x = landmarks.landmark[i].x\n",
    "                            y = landmarks.landmark[i].y\n",
    "                            data_aux.append(x)\n",
    "                            data_aux.append(y)\n",
    "\n",
    "                        data_point = np.array(data_aux).reshape(1, -1)\n",
    "                        label = model.predict(data_point)\n",
    "                        proba = model.predict_proba(data_point)\n",
    "                        max_proba = np.max(proba)\n",
    "\n",
    "                        with prediction_lock:\n",
    "                            latest_prediction = {\n",
    "                                'label': label[0],\n",
    "                                'confidence': max_proba * 100,\n",
    "                                'timestamp': current_time\n",
    "                            }\n",
    "\n",
    "                        print(f\"ðŸ¤– Prediction: {label[0]} ({max_proba*100:.2f}%)\")\n",
    "                        prediction_buffer.append(label[0])\n",
    "                        publish_majority_prediction()\n",
    "\n",
    "                last_processed_time = current_time\n",
    "            else:\n",
    "                time.sleep(0.01)\n",
    "        else:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "# ==== Start processing thread ====\n",
    "processing_thread = threading.Thread(target=process_frames)\n",
    "processing_thread.start()\n",
    "\n",
    "# ==== Connect to camera stream ====\n",
    "cap = cv2.VideoCapture(esp32_cam_url)\n",
    "if not cap.isOpened():\n",
    "    print(f\"Failed to connect to the ESP32-CAM at {esp32_cam_url}\")\n",
    "    processing_active = False\n",
    "    processing_thread.join()\n",
    "    exit()\n",
    "\n",
    "print(f\"âœ… Connected to ESP32-CAM at {esp32_cam_url}. Press 'q' to quit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        frame = cv2.resize(frame, (640, 480))\n",
    "        cv2.imshow('ESP32-CAM Stream', frame)\n",
    "\n",
    "        if len(frame_queue) > 0:\n",
    "            frame_queue.pop()\n",
    "        frame_queue.append(frame.copy())\n",
    "\n",
    "        with prediction_lock:\n",
    "            if latest_prediction:\n",
    "                # Optionally do something with it\n",
    "                pass\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    processing_active = False\n",
    "    processing_thread.join()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    client.loop_stop()\n",
    "    client.disconnect()\n",
    "    print(\"ðŸ“´ Disconnected from MQTT broker.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6ebe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
